{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/PhilippeCodes/Advanced-Regression-Techniques/blob/master/house-prices/House-prices%20Gradient%20Boosting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso, HuberRegressor, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "#from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "output_dir = os.path.join(cwd, \"../../data\")\n",
    "df = pd.read_csv(os.path.join(output_dir, \"combined_nodeinfo.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(output_dir, \"combined_nodeinfotest.csv\"))\n",
    "#df_test = pd.read_csv(os.path.join(output_dir, \"figlet1599056546.006096_1599063766.431225.csv\"))\n",
    "df = df.drop(df[df.responsetime > 2].index)\n",
    "df.pop('Time')\n",
    "df_test = df_test.drop(df_test[df_test.responsetime > 2].index)\n",
    "df_test.pop('Time')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = pd.DataFrame()\n",
    "df_ttest = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t['total_cpu_util'] = (df['totalcpuUtilization']*df['totalcpu'])/100\n",
    "df_t['total_mem_util'] = (df['totalmemoryUtilization']*df['totalmemory'])*1e-9\n",
    "df_t['responsetime'] = df['responsetime']\n",
    "df_t['requests'] = df['requests']\n",
    "df_t['replica'] = df['replica']\n",
    "df_ttest['total_cpu_util'] = (df_test['totalcpuUtilization']*df_test['totalcpu'])/100\n",
    "df_ttest['total_mem_util'] = (df_test['totalmemoryUtilization']*df_test['totalmemory'])*1e-9\n",
    "df_ttest['responsetime'] = df_test['responsetime']\n",
    "df_ttest['requests'] = df_test['requests']\n",
    "df_ttest['replica'] = df_test['replica']\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = len(df_t)\n",
    "#train_df = df_t[0:int(n*0.8)]\n",
    "#test_df = df_t[int(n*0.8):]\n",
    "x_train = df_t[[col for col in df_t.columns if col != 'requests']]\n",
    "y_train = df_t['requests']\n",
    "#x_train = preprocessing.normalize(x_train, norm='max')\n",
    "x_test = df_ttest[[col for col in df_ttest.columns if col != 'requests']]\n",
    "y_test = df_ttest['requests']\n",
    "#x_test = preprocessing.normalize(x_test, norm='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = train_df[[col for col in train_df.columns if col != 'responsetime']]\n",
    "#y_train = train_df['responsetime']\n",
    "\n",
    "#x_test = test_df[[col for col in test_df.columns if col != 'responsetime']]\n",
    "#y_test = test_df['responsetime']\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'fit_intercept':[True,False]}\n",
    "linear = GridSearchCV(LinearRegression(),\n",
    "                      parameters, \n",
    "                      cv=7,\n",
    "                      scoring='r2',\n",
    "                      n_jobs=-1)\n",
    "linear.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = linear.predict(x_train)\n",
    "y_te_pred = linear.predict(x_test)\n",
    "\n",
    "print('linear reg score on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('linear reg score on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', linear.best_score_)\n",
    "print('Best Params: ', linear.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolynomialRegression(**kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(), LinearRegression(**kwargs))\n",
    "degrees = [4,5]\n",
    "param_grid = {'polynomialfeatures__degree': degrees, 'linearregression__fit_intercept': [True, False]}\n",
    "polynomial = GridSearchCV(PolynomialRegression(),\n",
    "                      param_grid, \n",
    "                      cv=7,\n",
    "                      scoring='r2',\n",
    "                      n_jobs=-1)\n",
    "polynomial.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = polynomial.predict(x_train)\n",
    "y_te_pred = polynomial.predict(x_test)\n",
    "\n",
    "print('linear reg score on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('linear reg score on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', polynomial.best_score_)\n",
    "print('Best Params: ', polynomial.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-20,10,num=20)\n",
    "\n",
    "ridge_cv = GridSearchCV(Ridge(),\n",
    "                       {'alpha': alphas},\n",
    "                         cv=7, \n",
    "                         scoring='r2',\n",
    "                         n_jobs=-1)\n",
    "\n",
    "ridge_cv.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = ridge_cv.predict(x_train)\n",
    "y_te_pred = ridge_cv.predict(x_test)\n",
    "\n",
    "print('RMSLE on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('RMSLE on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', ridge_cv.best_score_)\n",
    "print('Best Params: ', ridge_cv.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = GridSearchCV(Lasso(),\n",
    "                       {'alpha': alphas},\n",
    "                         cv=7, \n",
    "                        scoring='r2',\n",
    "                         n_jobs=-1)\n",
    "\n",
    "lasso_cv.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = lasso_cv.predict(x_train)\n",
    "y_te_pred = lasso_cv.predict(x_test)\n",
    "\n",
    "print('RMSLE on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('RMSLE on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', lasso_cv.best_score_)\n",
    "print('Best Params: ', lasso_cv.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    " \n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(4, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "epochs = [5, 10, 30]\n",
    "batches = [100, 150]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                   cv=5, \n",
    "                         scoring='r2',\n",
    "                         n_jobs=-1,\n",
    "                   verbose=10)\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search and Deeplearning models in python with keras\n",
    "\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60]\n",
    "epochs = [10, 50]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the learning rate and momentum\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import SGD\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate, momentum=momentum)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the weight initialization\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(init_mode='uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, kernel_initializer=init_mode, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the activation function\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, kernel_initializer='uniform', activation=activation))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the dropout rate\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the number of neurons\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(neurons=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=4, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=10)\n",
    "# define the grid search parameters\n",
    "neurons = [5, 6, 10]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5, verbose=10, scoring='r2')\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huber Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = np.arange(1, 1.5, 0.05)\n",
    "\n",
    "\n",
    "huber_cv = GridSearchCV(HuberRegressor(),\n",
    "                       {'epsilon': epsilon,\n",
    "                        'alpha': alphas},\n",
    "                         cv=7, \n",
    "                         scoring='r2',\n",
    "                         n_jobs=-1)\n",
    "\n",
    "huber_cv.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = huber_cv.predict(x_train)\n",
    "y_te_pred = huber_cv.predict(x_test)\n",
    "\n",
    "print('RMSLE on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('RMSLE on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', huber_cv.best_score_)\n",
    "print('Best Params: ', huber_cv.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {  \n",
    "                 'min_samples_leaf': [2, 4, 6],\n",
    "                 'max_depth': [10, 15, 20],\n",
    "                 'n_estimators': [100, 150, 200],\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV( RandomForestRegressor(),\n",
    "                      param_grid=parameters, \n",
    "                      cv=7, \n",
    "                      scoring='r2',\n",
    "                      n_jobs=-1)\n",
    "\n",
    "rf_cv.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = rf_cv.predict(x_train)\n",
    "y_te_pred = rf_cv.predict(x_test)\n",
    "\n",
    "print('RMSLE on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('RMSLE on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', rf_cv.best_score_)\n",
    "print('Best Params: ', rf_cv.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalrf_cv = RandomForestRegressor(max_depth=20, min_samples_leaf=4, n_estimators=150)\n",
    "finalrf_cv.fit(x_train, y_train)\n",
    "y_tr_pred = finalrf_cv.predict(x_train)\n",
    "y_te_pred = finalrf_cv.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('Test R² :',R)\n",
    "\n",
    "R = r2_score(y_train, y_tr_pred)\n",
    "print ('Train R² :',R)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal alpha with grid search\n",
    "alpha = np.logspace(-20,10,num=20)\n",
    "l1_ratio = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "param_grid = dict(alpha=alpha, l1_ratio=l1_ratio)\n",
    "grid = GridSearchCV(ElasticNet(), \n",
    "                    param_grid=param_grid,\n",
    "                    cv=7,\n",
    "                    scoring='r2', \n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = grid.predict(x_train)\n",
    "y_te_pred = grid.predict(x_test)\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {  \n",
    "                 'learning_rate': [0.02],\n",
    "                 'min_samples_split': [10, 15, 20],\n",
    "                 'min_samples_leaf': [10, 15, 20],\n",
    "                 'max_depth': [3, 4, 5],\n",
    "                 'n_estimators': [1000, 1500, 2000],\n",
    "                 'loss': ['huber'],\n",
    "                 'max_features': ['sqrt']\n",
    "             }\n",
    "\n",
    "\n",
    "GBoost_cv = GridSearchCV(GradientBoostingRegressor(), \n",
    "                         param_grid=parameters, \n",
    "                         n_jobs=-1, \n",
    "                         scoring='r2',\n",
    "                         cv=5)\n",
    "\n",
    "\n",
    "GBoost_cv.fit(x_train, y_train)\n",
    "\n",
    "y_tr_pred = GBoost_cv.predict(x_train)\n",
    "y_te_pred = GBoost_cv.predict(x_test)\n",
    "\n",
    "print('RMSLE on our train data: {:.3f}'.format(sqrt(mse(y_train, y_tr_pred))))\n",
    "print('RMSLE on our test data: {:.3f}'.format(sqrt(mse(y_test, y_te_pred))))\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "R = r2_score(y_test, y_te_pred)\n",
    "print ('R² :',R)\n",
    "print('Best Score: ', GBoost_cv.best_score_)\n",
    "print('Best Params: ', GBoost_cv.best_params_)\n",
    "\n",
    "plt.scatter(range(0,len(y_test)),y_test, color='red')\n",
    "plt.scatter(range(0,len(y_te_pred)),y_te_pred, color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
